{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d5fe4f-675c-4708-9267-1549d4cbff6a",
   "metadata": {},
   "source": [
    "# Physically Recurrent Neural Networks - Demo notebook\n",
    "\n",
    "This notebook provides a minimal demonstration of training a PRNN.\n",
    "It is assumed that the reader is familiar with the torch-based implementation.\n",
    "\n",
    "This Jax-based code should result in >10x faster training times, as demonstrated by the first example code block, which is very similar to the example in the torch-based code.\n",
    "\n",
    "The final two code blocks show learning curves comparing two different decoder layers.\n",
    "\n",
    "## Load packages, get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0c6d9-ed79-4572-b702-4b84d2ed6992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T12:43:21.194064Z",
     "start_time": "2025-05-06T12:43:21.171593Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib widget\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')  # force jax to use cpu; often faster.\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from trainer import Trainer\n",
    "from utils import StressStrainDataset\n",
    "\n",
    "from prnn import create_prnn_model\n",
    "\n",
    "# Download and unzip the same dataset as for the torch-based prnn\n",
    "if not os.path.isdir('datasets'):\n",
    "    print('Downloading and unzipping datasets...')\n",
    "    urlretrieve('https://surfdrive.surf.nl/files/index.php/s/OcSDq0zNqkVbvIO/download', 'datasets.zip')\n",
    "    zip_file = zipfile.ZipFile('datasets.zip')\n",
    "    zip_file.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38603a26-19f6-4e26-a9b5-6d93d5b52640",
   "metadata": {},
   "source": [
    "## Example code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1105a-5a3f-49a4-8860-7484ddf06b1d",
   "metadata": {},
   "source": [
    "### Train a PRNN model on 18 simple paths for 20 epochs, to compare the computational speed to the torch-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb9d00-ef6c-4ea9-978a-22c10c61f8fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T12:43:26.251513Z",
     "start_time": "2025-05-06T12:43:21.225493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "settings = {\n",
    "    'data_path': 'datasets/canonical.data',\n",
    "    'seq_length': 60,\n",
    "    'train_batch_size': 3,\n",
    "    'decoder_type': 'SoftLayer',\n",
    "\n",
    "    'input_norm': False,\n",
    "    'output_norm': False,\n",
    "\n",
    "    'mat_points': 2,\n",
    "    'max_epochs': 20,\n",
    "    'learning_rate': 1e-1,\n",
    "    'patience': 50,                     # early stopping epochs\n",
    "    'interval': 1,                      # interval for which we compute validation loss (once every x epochs)\n",
    "\n",
    "    'feature_dim': 3,\n",
    "    'verbose': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Setup Keys and Seed\n",
    "key = jax.random.PRNGKey(settings['seed'])\n",
    "np.random.seed(settings['seed']) # For NumPy-based shuffling if used\n",
    "random.seed(settings['seed']) # For Python random if used\n",
    "\n",
    "\n",
    "dataset = StressStrainDataset(settings['data_path'], [0,1,2], [3,4,5], seq_length=settings['seq_length'])\n",
    "all_samples = dataset.get_all_batches()\n",
    "\n",
    "# the prnn parameters and material properties are explicitly passed around\n",
    "prnn, params, material = create_prnn_model(random_key=key, n_matpts=settings['mat_points'], decoder_type=settings['decoder_type'])\n",
    "\n",
    "# Initialize trainer class\n",
    "train_handler = Trainer(prnn, params, material=material, random_key=key, **settings)\n",
    "\n",
    "# Use all samples for training & validation for demonstration (effectively disabling early stopping)\n",
    "start_time = time.time()\n",
    "train_handler.train(all_samples, all_samples, **settings)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training for 20 epochs: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a59463-9a13-461a-a4e1-157c2d4c8814",
   "metadata": {},
   "source": [
    "### Train thirty PRNNs from scratch and plot a learning curve\n",
    "This will take approximately 10-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bde5dc-d529-42d0-8b40-1afe6a603f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T12:47:39.616397Z",
     "start_time": "2025-05-06T12:45:57.802524Z"
    }
   },
   "outputs": [],
   "source": [
    "base_settings = {\n",
    "    'data_path': 'datasets/gpCurves.data',\n",
    "    'seq_length': 60,\n",
    "    'train_samples': 80,\n",
    "    'train_batch_size': 4,\n",
    "    'decoder_type': 'SoftLayer',\n",
    "\n",
    "    'input_norm': False,   # Note: keep false. Normalization has not yet been consistently implemented for computing losses.\n",
    "    'output_norm': False,\n",
    "\n",
    "    'mat_points': 2,\n",
    "\n",
    "    'max_epochs': 10000,\n",
    "    'learning_rate': 1e-2,              # Note that setting a constant lr is not ideal\n",
    "    'patience': 50,                     # early stopping epochs\n",
    "    'interval': 1,                      # interval for which we compute validation loss (once every x epochs)\n",
    "\n",
    "    'feature_dim': 3,\n",
    "    'verbose': False,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Number of curves\n",
    "ncurves = [1, 2, 3, 4, 5, 6, 8, 10, 12, 16]\n",
    "nmodels = 3\n",
    "\n",
    "# Split dataset manually\n",
    "dataset = StressStrainDataset(base_settings['data_path'], [0,1,2], [3,4,5], seq_length=base_settings['seq_length'])\n",
    "all_samples = dataset.get_all_batches()\n",
    "num_samples = len(dataset)\n",
    "all_indices = np.arange(num_samples)\n",
    "train_indices = all_indices[:40]\n",
    "val_indices = all_indices[40:70]\n",
    "test_indices = all_indices[70:]\n",
    "val_dataset = all_samples[val_indices]\n",
    "test_dataset = all_samples[test_indices]\n",
    "\n",
    "seed = base_settings['seed']\n",
    "test_losses = np.zeros((len(ncurves), nmodels))\n",
    "\n",
    "for run_i in range(nmodels):\n",
    "    for ncurve in ncurves:\n",
    "        print(f\"\\n--- Running with {ncurve} training samples, run {run_i + 1}/{nmodels} ---\")\n",
    "        # Setup Keys and Seed\n",
    "        seed += 1\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        np.random.seed(seed) # For NumPy-based shuffling if used\n",
    "        random.seed(seed) # For Python random if used\n",
    "\n",
    "        # Copy and modify settings for this run\n",
    "        settings = base_settings.copy()\n",
    "        settings['train_samples'] = ncurve\n",
    "        settings['train_batch_size'] = min(max(1, ncurve // 4), 4)        # minimum=1, maximum=4, else num_samples // 4\n",
    "        settings['seed'] = seed\n",
    "\n",
    "        # Create random training subset\n",
    "        cur_indices = train_indices.copy()\n",
    "        np.random.shuffle(cur_indices)\n",
    "        cur_indices = cur_indices[:settings['train_samples']]\n",
    "        train_dataset = all_samples[cur_indices]\n",
    "\n",
    "        # the prnn parameters and material properties are explicitly passed around\n",
    "        prnn, params, material = create_prnn_model(random_key=key, n_matpts=settings['mat_points'], decoder_type=settings['decoder_type'])\n",
    "        \n",
    "        # Initialize trainer class\n",
    "        train_handler = Trainer(prnn, params, material=material, random_key=key, **settings)\n",
    "\n",
    "        # Train the model\n",
    "        train_handler.train(train_dataset, val_dataset, **settings)\n",
    "        print(f\"Model trained for {train_handler._epoch} epochs.\")\n",
    "\n",
    "        # Evaluate the test loss (Note, this uses the L2 norm, the torch-based notebook used the L1 norm)\n",
    "        test_losses[ncurves.index(ncurve), run_i] = train_handler._eval_step_jit(train_handler._state, test_dataset, material)\n",
    "\n",
    "plt.figure()\n",
    "ncurves_arr = np.array(ncurves)\n",
    "ncurves_arr = np.repeat(ncurves_arr[:, np.newaxis], nmodels, axis=1)\n",
    "plt.scatter(ncurves_arr,test_losses)\n",
    "plt.ylabel('L2 Loss')\n",
    "plt.xlabel('n curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71e5e3-ba98-4930-9dd8-7f3d165395ec",
   "metadata": {},
   "source": [
    "### Repeat the learning curve experiment with a custom sparse decoder layer\n",
    "With a custom decoder layer that connects component-wise, and where the weights sum to one, we should be able to use even fewer training samples to obtain good performance when using more material points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff6971cc5ce5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_base_settings = base_settings.copy()\n",
    "new_base_settings['decoder_type'] = 'SparseNormLayer'\n",
    "\n",
    "seed = new_base_settings['seed']\n",
    "test_losses_sparse_norm = np.zeros((len(ncurves), nmodels))\n",
    "\n",
    "for run_i in range(nmodels):\n",
    "    for ncurve in ncurves:\n",
    "        print(f\"\\n--- Running with {ncurve} training samples, run {run_i + 1}/{nmodels} ---\")\n",
    "        # Setup Keys and Seed\n",
    "        seed += 1\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        np.random.seed(seed) # For NumPy-based shuffling if used\n",
    "        random.seed(seed) # For Python random if used\n",
    "\n",
    "        # Copy and modify settings for this run\n",
    "        settings = new_base_settings.copy()\n",
    "        settings['train_samples'] = ncurve\n",
    "        settings['train_batch_size'] = min(max(1, ncurve // 4), 4)        # minimum=1, maximum=4, else num_samples // 4\n",
    "        settings['seed'] = seed\n",
    "\n",
    "        # Create random training subset\n",
    "        cur_indices = train_indices.copy()\n",
    "        np.random.shuffle(cur_indices)\n",
    "        cur_indices = cur_indices[:settings['train_samples']]\n",
    "        train_dataset = all_samples[cur_indices]\n",
    "\n",
    "        # Create and train the model\n",
    "        prnn, params, material = create_prnn_model(random_key=key, n_matpts=settings['mat_points'], decoder_type=settings['decoder_type'])\n",
    "        train_handler = Trainer(prnn, params, material=material, random_key=key, **settings)\n",
    "        train_handler.train(train_dataset, val_dataset, **settings)\n",
    "        print(f\"Model trained for {train_handler._epoch} epochs.\")\n",
    "\n",
    "        test_losses_sparse_norm[ncurves.index(ncurve), run_i] = train_handler._eval_step_jit(train_handler._state, test_dataset, material)\n",
    "\n",
    "plt.figure()\n",
    "ncurves_arr = np.array(ncurves)\n",
    "ncurves_arr = np.repeat(ncurves_arr[:, np.newaxis], nmodels, axis=1)\n",
    "plt.scatter(ncurves_arr,test_losses, label='Base')\n",
    "plt.scatter(ncurves_arr,test_losses_sparse_norm, label='New decoder')\n",
    "plt.ylabel('L2 Loss')\n",
    "plt.xlabel('n curves')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
